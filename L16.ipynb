{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OydShklHHtdh"
   },
   "source": [
    "In this notebook, we'll build a model to classify online posts about baseball and hockey.\n",
    "\n",
    "Below we download the online posts data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ofSSY-oGw8rw"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "groups = ['rec.sport.baseball', 'rec.sport.hockey']\n",
    "# newsgroups is an instance of sklearn.utils.Bunch object.\n",
    "# * newsgroups.DESCR describing the dataset.\n",
    "# * newsgroups.data is a list of length 1993, each element a str representing one post.\n",
    "# * newsgroups.target is a one-dimensional numpy.ndarray of length 1993, each element an int64\n",
    "#   with value of either 0 (rec.sport.baseball) or 1 (rec.sport.hockey').\n",
    "#   There are 994 instances of 0, and 999 instances of 1.\n",
    "newsgroups = fetch_20newsgroups(subset='all', remove = ('headers', 'footers', 'quotes'), categories = groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tbhmnw-Ewyk7"
   },
   "outputs": [],
   "source": [
    "# from sklearn.naive_bayes import MultinomialNB\n",
    "# from sklearn import metrics\n",
    "# newsgroups_test = fetch_20newsgroups(subset='test', remove = ['headers', 'footers', 'quotes'], categories = groups)\n",
    "# vectors_test = vectorizer.transform(newsgroups_test.data)\n",
    "# clf = MultinomialNB(alpha=.01)\n",
    "# clf.fit(vectors, newsgroups_train.target)\n",
    "# pred = clf.predict(vectors_test)\n",
    "# metrics.f1_score(newsgroups_test.target, pred, average='macro')\n",
    "# 0.88213592402729568 (full set)\n",
    "# 0.9320767597087378"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t4GDNzTYIqso"
   },
   "source": [
    "Next, we download GloVe vectors we will be using to represent our post data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sCjJULtUqjAe",
    "outputId": "d8d0824c-86d0-46bd-cdd5-b6bc97c9861c"
   },
   "outputs": [],
   "source": [
    "import os.path\n",
    "if not os.path.exists('glove.6B.zip'):\n",
    "    import urllib.request\n",
    "    data = urllib.request.urlopen('http://nlp.stanford.edu/data/glove.6B.zip').read()\n",
    "    with open('glove.6B.zip', 'wb') as f:\n",
    "        f.write(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GZsutIRzJCzg"
   },
   "source": [
    "Below we unzip the GloVe file we downloaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5j_YyS37qv7m",
    "outputId": "74348adc-9e20-4738-b763-f41e182e901a"
   },
   "outputs": [],
   "source": [
    "import os.path\n",
    "if not os.path.exists('glove.6B.100d.txt'):\n",
    "    import zipfile\n",
    "    with zipfile.ZipFile('glove.6B.zip') as f:\n",
    "        f.extractall()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lmi-6INoJHbv"
   },
   "source": [
    "Next, we load the GloVe vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "siJVA4bpyPY8",
    "outputId": "fee90218-af69-4565-d006-3af2f9965dc8"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "embeddings_index = {}\n",
    "# glove.6B.100d.txt is a text file of 400,000 lines.\n",
    "# Each line has 101 tokens. The first token is a word, and the remaining tokens are\n",
    "# floating point numbers between -1 and 1, containing the feature vector for the word.\n",
    "f = open('glove.6B.100d.txt', encoding='utf-8')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "# embeddings_index is dict[str, numpy.ndarray], mapping words to arrays of 100 float32 values each.\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some example word similarities.\n",
    "def similar(a, b):\n",
    "    # Adapted from L16qs.ipynb.\n",
    "    print(\n",
    "        a, b,\n",
    "        (embeddings_index[a] @ embeddings_index[b])\n",
    "            / np.sqrt(embeddings_index[a] @ embeddings_index[a]) \n",
    "            / np.sqrt(embeddings_index[b] @ embeddings_index[b]))\n",
    "\n",
    "similar('cat', 'dog')     # 0.8798\n",
    "similar('cat', 'person')  # 0.3757\n",
    "similar('up', 'down')     # 0.9160\n",
    "similar('the', 'a')       # 0.7760\n",
    "similar('the', 'up')      # 0.7415"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zzij4IlhJMsE"
   },
   "source": [
    "Next, we convert the data to a collection of word GloVe word vectors for each of the words in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 550
    },
    "id": "CB7FMHUsI4WD",
    "outputId": "39ba0ad9-ba9d-4d20-be20-d112de6bac9f"
   },
   "outputs": [],
   "source": [
    "# !pip install keras=='2.3.1'\n",
    "import tensorflow as tf\n",
    "# from tf.keras.preprocessing.text import Tokenizer\n",
    "# from keras.preprocessing.sequence import pad_sequences\n",
    "# from keras.utils import to_categorical\n",
    "\n",
    "print('Preparing embedding matrix.')\n",
    "MAX_NUM_WORDS = 20000\n",
    "EMBEDDING_DIM = 100\n",
    "\n",
    "# Tokenizer will break text in newsgroups.data into words, and replace each word by an index.\n",
    "# * word_index is a dict[str, int] mapping each word to its index. The index starts from 0\n",
    "#   and then increases sequentially, with more frequent words having smaller indexes.\n",
    "# * tokenizer.index_word (unused here) is the opposite of word_index, a dict[int, str]\n",
    "#   mapping each index to the corresponding word.\n",
    "# * sequences is list[list[int]], containing word indexes for each article in newsgroups.data.\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=MAX_NUM_WORDS)\n",
    "tokenizer.fit_on_texts(newsgroups.data)\n",
    "sequences = tokenizer.texts_to_sequences(newsgroups.data)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "# prepare embedding matrix\n",
    "num_words = min(MAX_NUM_WORDS, len(word_index) + 1)\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if i >= MAX_NUM_WORDS:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "# embedding_matrix has the same contents as embeddings_index, except it is a 2-d matrix\n",
    "# instead of a dict. Each row of the matrix contains the feature vector of one word.\n",
    "# The row number corresponds to the index from word_index. Any words that are in\n",
    "# embeddings_index but not in word_index are omitted from embedding_matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BuuAUWLxJqe-"
   },
   "source": [
    "Next, we'll build our dataset for training, `data` and `labels`, as well as our test set, `data_test` and `labels_test`.  We will limit our training set to 200 examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "71Npelu5XOXI"
   },
   "outputs": [],
   "source": [
    "# from keras.preprocessing.text import Tokenizer\n",
    "# from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# finally, vectorize the text samples into a 2D integer tensor\n",
    "# * sequence (list[list[int]]) => data (numpy.ndarray of shape (1993, 1000))\n",
    "# * newsgroups.target (numpy.ndarray of shape (1993,)) => labels (numpy.ndarray of shape (1993, 2))\n",
    "#   A 0 in newsgroups.target becomes a [1., 0.] in labels, and a 1 becomes a [0., 1.].\n",
    "MAX_SEQUENCE_LENGTH = 1000\n",
    "data = tf.keras.preprocessing.sequence.pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "labels = to_categorical(np.asarray(newsgroups.target))\n",
    "\n",
    "print('Before split:')\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)\n",
    "\n",
    "data, data_test, labels, labels_test = train_test_split(data,labels,train_size=200)\n",
    "\n",
    "print()\n",
    "print('After split:')\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)\n",
    "print('Shape of data_test tensor:', data_test.shape)\n",
    "print('Shape of label_test tensor:', labels_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Q80Hv4ZMM14"
   },
   "source": [
    "Next, we'll declare a `train` function that declares and trains the model with `pretrain` weights.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MG5dKkuxW95S"
   },
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Input, GlobalMaxPooling1D\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
    "from keras.models import Model\n",
    "from keras.initializers import Constant\n",
    "# from keras.optimizers import RMSprop\n",
    "# from keras.optimizers import Adam\n",
    "from keras import optimizers\n",
    "\n",
    "# load pre-trained word embeddings into an Embedding layer\n",
    "# note that we set trainable = False so as to keep the embeddings fixed\n",
    "# num_words = len(vectorizer.vocabulary_)\n",
    "# num_words = len(word_index)+1\n",
    "\n",
    "def train(pretrain):\n",
    "  if not pretrain:  # train your own embedding\n",
    "    embedding_layer = Embedding(num_words,\n",
    "                              EMBEDDING_DIM,\n",
    "                              input_length=MAX_SEQUENCE_LENGTH,\n",
    "                              trainable=True\n",
    "                             )\n",
    "  else:\n",
    "      embedding_layer = Embedding(num_words,\n",
    "                              EMBEDDING_DIM,\n",
    "                              embeddings_initializer=Constant(embedding_matrix),\n",
    "                              input_length=MAX_SEQUENCE_LENGTH,\n",
    "                              trainable=False\n",
    "                           )\n",
    "  print('Training model.')\n",
    "\n",
    "  # train a 1D convnet with global maxpooling\n",
    "  sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "\n",
    "  embedded_sequences = embedding_layer(sequence_input)\n",
    "  x = Conv1D(128, 5, activation='relu')(embedded_sequences)\n",
    "  x = MaxPooling1D(5)(x)\n",
    "  x = Conv1D(128, 5, activation='relu')(x)\n",
    "  x = MaxPooling1D(5)(x)\n",
    "  x = Conv1D(128, 5, activation='relu')(x)\n",
    "  x = GlobalMaxPooling1D()(x)\n",
    "  x = Dense(128, activation='relu')(x)\n",
    "  preds = Dense(len(groups), activation='softmax')(x)\n",
    "\n",
    "  solver = optimizers.Adam(learning_rate=0.0005, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
    "\n",
    "  model = Model(sequence_input, preds)\n",
    "  model.compile(loss='categorical_crossentropy',\n",
    "                optimizer=solver,\n",
    "                metrics=['acc'])\n",
    "\n",
    "  model.fit(data, labels,\n",
    "            epochs=100,\n",
    "            validation_data=(data_test, labels_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Ufl-EMEMn7L"
   },
   "source": [
    "Below we train the model without pretrained weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cBFzGzyqXKGk"
   },
   "outputs": [],
   "source": [
    "train(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wFgJVqEjMsaR"
   },
   "source": [
    "Next we train the model with pretrained weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ni_xRrAmdN-X"
   },
   "outputs": [],
   "source": [
    "train(True)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "L16.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
