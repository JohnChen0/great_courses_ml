{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OydShklHHtdh"
   },
   "source": [
    "In this notebook, we'll build a model to classify online posts about baseball and hockey.\n",
    "\n",
    "Below we download the online posts data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ofSSY-oGw8rw"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "groups = ['rec.sport.baseball', 'rec.sport.hockey']\n",
    "newsgroups = fetch_20newsgroups(subset='all', remove = ['headers', 'footers', 'quotes'], categories = groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tbhmnw-Ewyk7"
   },
   "outputs": [],
   "source": [
    "# from sklearn.naive_bayes import MultinomialNB\n",
    "# from sklearn import metrics\n",
    "# newsgroups_test = fetch_20newsgroups(subset='test', remove = ['headers', 'footers', 'quotes'], categories = groups)\n",
    "# vectors_test = vectorizer.transform(newsgroups_test.data)\n",
    "# clf = MultinomialNB(alpha=.01)\n",
    "# clf.fit(vectors, newsgroups_train.target)\n",
    "# pred = clf.predict(vectors_test)\n",
    "# metrics.f1_score(newsgroups_test.target, pred, average='macro')\n",
    "# 0.88213592402729568 (full set)\n",
    "# 0.9320767597087378"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t4GDNzTYIqso"
   },
   "source": [
    "Next, we download GloVe vectors we will be using to represent our post data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sCjJULtUqjAe",
    "outputId": "d8d0824c-86d0-46bd-cdd5-b6bc97c9861c"
   },
   "outputs": [],
   "source": [
    "!wget http://nlp.stanford.edu/data/glove.6B.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GZsutIRzJCzg"
   },
   "source": [
    "Below we unzip the GloVe file we downloaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5j_YyS37qv7m",
    "outputId": "74348adc-9e20-4738-b763-f41e182e901a"
   },
   "outputs": [],
   "source": [
    "!unzip glove.6B.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lmi-6INoJHbv"
   },
   "source": [
    "Next, we load the GloVe vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "siJVA4bpyPY8",
    "outputId": "fee90218-af69-4565-d006-3af2f9965dc8"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "embeddings_index = {}\n",
    "f = open('glove.6B.100d.txt')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zzij4IlhJMsE"
   },
   "source": [
    "Next, we convert the data to a collection of word GloVe word vectors for each of the words in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 550
    },
    "id": "CB7FMHUsI4WD",
    "outputId": "39ba0ad9-ba9d-4d20-be20-d112de6bac9f"
   },
   "outputs": [],
   "source": [
    "# !pip install keras=='2.3.1'\n",
    "import tensorflow as tf\n",
    "# from tf.keras.preprocessing.text import Tokenizer\n",
    "# from keras.preprocessing.sequence import pad_sequences\n",
    "# from keras.utils import to_categorical\n",
    "\n",
    "print('Preparing embedding matrix.')\n",
    "MAX_NUM_WORDS = 20000\n",
    "MAX_SEQUENCE_LENGTH = 1000\n",
    "EMBEDDING_DIM = 100\n",
    "\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=MAX_NUM_WORDS)\n",
    "tokenizer.fit_on_texts(newsgroups.data)\n",
    "sequences = tokenizer.texts_to_sequences(newsgroups.data)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "# prepare embedding matrix\n",
    "num_words = min(MAX_NUM_WORDS, len(word_index) + 1)\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if i >= MAX_NUM_WORDS:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BuuAUWLxJqe-"
   },
   "source": [
    "Next, we'll build our dataset for training, `data` and `labels`, as well as our test set, `data_test` and `labels_test`.  We will limit our training set to 200 examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "71Npelu5XOXI"
   },
   "outputs": [],
   "source": [
    "# from keras.preprocessing.text import Tokenizer\n",
    "# from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# finally, vectorize the text samples into a 2D integer tensor\n",
    "MAX_NUM_WORDS = 20000\n",
    "MAX_SEQUENCE_LENGTH = 1000\n",
    "\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=MAX_NUM_WORDS)\n",
    "tokenizer.fit_on_texts(newsgroups.data)\n",
    "sequences = tokenizer.texts_to_sequences(newsgroups.data)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "data = tf.keras.preprocessing.sequence.pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "labels = to_categorical(np.asarray(newsgroups.target))\n",
    "\n",
    "# print(data.shape)\n",
    "\n",
    "data, data_test, labels, labels_test = train_test_split(data,labels,train_size=200)\n",
    "\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)\n",
    "print('Shape of data_test tensor:', data_test.shape)\n",
    "print('Shape of label_test tensor:', labels_test.shape)\n",
    "\n",
    "# split the data into a training set and a validation set\n",
    "# indices = np.arange(data.shape[0])\n",
    "# np.random.shuffle(indices)\n",
    "# data = data[indices]\n",
    "# labels = labels[indices]\n",
    "# num_validation_samples = int(VALIDATION_SPLIT * data.shape[0])\n",
    "\n",
    "# x_train = data[:-num_validation_samples]\n",
    "# y_train = labels[:-num_validation_samples]\n",
    "# x_val = data[-num_validation_samples:]\n",
    "# y_val = labels[-num_validation_samples:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Q80Hv4ZMM14"
   },
   "source": [
    "Next, we'll declare a `train` function that declares and trains the model with `pretrain` weights.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MG5dKkuxW95S"
   },
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Input, GlobalMaxPooling1D\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
    "from keras.models import Model\n",
    "from keras.initializers import Constant\n",
    "# from keras.optimizers import RMSprop\n",
    "# from keras.optimizers import Adam\n",
    "from keras import optimizers\n",
    "\n",
    "EMBEDDING_DIM = 100\n",
    "\n",
    "# load pre-trained word embeddings into an Embedding layer\n",
    "# note that we set trainable = False so as to keep the embeddings fixed\n",
    "# num_words = len(vectorizer.vocabulary_)\n",
    "# num_words = len(word_index)+1\n",
    "\n",
    "def train(pretrain):\n",
    "  if not pretrain:  # train your own embedding\n",
    "    embedding_layer = Embedding(num_words,\n",
    "                              EMBEDDING_DIM,\n",
    "                              input_length=MAX_SEQUENCE_LENGTH,\n",
    "                              trainable=True\n",
    "                             )\n",
    "  else:\n",
    "      embedding_layer = Embedding(num_words,\n",
    "                              EMBEDDING_DIM,\n",
    "                              embeddings_initializer=Constant(embedding_matrix),\n",
    "                              input_length=MAX_SEQUENCE_LENGTH,\n",
    "                              trainable=False\n",
    "                           )\n",
    "  print('Training model.')\n",
    "\n",
    "  # train a 1D convnet with global maxpooling\n",
    "  sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "\n",
    "  embedded_sequences = embedding_layer(sequence_input)\n",
    "  x = Conv1D(128, 5, activation='relu')(embedded_sequences)\n",
    "  x = MaxPooling1D(5)(x)\n",
    "  x = Conv1D(128, 5, activation='relu')(x)\n",
    "  x = MaxPooling1D(5)(x)\n",
    "  x = Conv1D(128, 5, activation='relu')(x)\n",
    "  x = GlobalMaxPooling1D()(x)\n",
    "  x = Dense(128, activation='relu')(x)\n",
    "  preds = Dense(len(groups), activation='softmax')(x)\n",
    "\n",
    "  solver = optimizers.Adam(lr=0.0005, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
    "\n",
    "  model = Model(sequence_input, preds)\n",
    "  model.compile(loss='categorical_crossentropy',\n",
    "                optimizer=solver,\n",
    "                metrics=['acc'])\n",
    "\n",
    "  model.fit(data, labels,\n",
    "            epochs=100,\n",
    "            validation_data=(data_test, labels_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Ufl-EMEMn7L"
   },
   "source": [
    "Below we train the model without pretrained weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cBFzGzyqXKGk"
   },
   "outputs": [],
   "source": [
    "train(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wFgJVqEjMsaR"
   },
   "source": [
    "Next we train the model with pretrained weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ni_xRrAmdN-X"
   },
   "outputs": [],
   "source": [
    "train(True)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "L16.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
