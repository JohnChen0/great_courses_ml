{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_ZnbwJGToM_l"
   },
   "source": [
    "In this notebook, we train a model to learn a strategy for Blackjack using a decision tree regression.\n",
    "\n",
    "Below we write an `actionpicker` function that decides what `action` to play given the `observation` from the Blackjack simulator, `env`.\n",
    "`epsilon` percent of the time, we decide on a random `action`.\n",
    "\n",
    "We also write a `train` function fits our decision tree regression to our `dat` and `labs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9nI21j7xoDA6"
   },
   "outputs": [],
   "source": [
    "def actionpicker(clf, env, observation, epsilon):\n",
    "  if clf == 0:\n",
    "    action = env.action_space.sample()\n",
    "  else:\n",
    "    pred = clf.predict([observation + (0,), observation + (1,)])\n",
    "    action = 1*(pred[1]>pred[0])\n",
    "  if random.random() < epsilon:\n",
    "    action = env.action_space.sample()\n",
    "  return(action)\n",
    "\n",
    "def train(dat, lab):\n",
    "  clf = tree.DecisionTreeRegressor(max_leaf_nodes = 6)\t\n",
    "  clf = clf.fit(dat, lab)\n",
    "  return(clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using Blackjack environment provided by gym API.\n",
    "It is a simplified Blackjack, where the player can only hit or stay at each turn.\n",
    "Other actions, such as split or double down, are not supported.\n",
    "Each card draw is from a fresh deck -- the API doesn't keep track of which cards have already been drawn.\n",
    "\n",
    "To create a new Blackjack environment:\n",
    "```\n",
    "env = gym.make(\"Blackjack-v0\", natural=False)\n",
    "```\n",
    "The optional parameter `natural` (which must be a keyword parameter instead of positional) determines whether 1.5 time reward is given to a user blackjack. By default no extra reward is given to blackjack.\n",
    "\n",
    "The Blackjack environment has three primary methods:\n",
    "- `env.reset() -> tuple[int, int, bool]` starts a new round, dealing two cards to the user and two cards to the dealer.\n",
    "  It returns a 3-tuple: total value of user's cards (with Ace valued at 11 if possible),\n",
    "  value of dealer's first card (dealer's second card is hidden at this point),\n",
    "  and whether the user has an Ace that is currently used as 11.\n",
    "  For the value of dealer's first card, Ace is represented by 1, regardless of whether it should be used as 11 or not.\n",
    "- `env.action_space.sample() -> int` randomly selects an action for the user.\n",
    "  It returns 0 (meaning stay) or 1 (meaning hit) with 50% probability each.\n",
    "- `env.step(action: int) -> tuple[tuple[int, int, bool], float, bool, dict]` takes one user action (either 0 for stay, or 1 for hit) and completes it.\n",
    "  Returns a 4-tuple. Item 0 is observation, a 3-tuple with the same meaning as that returned by `env.reset`;\n",
    "  item 1 is reward, amount won by the user; item 2 is done, indicating whether this round is done or not;\n",
    "  item 4 is an empty dict, probably for future use. `reward` is 0.0 if `done` is False.\n",
    "  - When `action` is 1, the code draws one card for the user. If the user is bust, the round completes and no action is possible until the env resets. Otherwise, the round is not complete, and the user can make another action.\n",
    "  - When `action` is 0, the code draws cards for dealer until the total value reaches at least 17, and the end round is complete.\n",
    "\n",
    "There are two attributes, `env.player` and `env.dealer`, that are useful for debugging. The return a list of all the cards currently held by the player and the dealer, respectively. Note that in a real game, the player can't see `env.dealer[1:]` while makeing a decision.\n",
    "\n",
    "The code below also calls `env.close()`, which actually does nothing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Li0Pfimp_eT"
   },
   "source": [
    "In the next code block, we train our Blackjack playing model and visualize the descision tree regression we have learned.  We will train our model 5 times, playing 100,000 rounds, and print the number of rounds it won each time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WUzI-EbQX-xJ"
   },
   "outputs": [],
   "source": [
    "# action 0 is stay\n",
    "# action 1 is hit\n",
    "\n",
    "import gym\n",
    "from sklearn import tree\n",
    "import random\n",
    "\n",
    "env = gym.make(\"Blackjack-v0\")\n",
    "epochs = 5\n",
    "N = 100000\n",
    "epsilon = 0.1\n",
    "clf = 0\n",
    "\n",
    "# In epoch 0, the player always chooses random actions.\n",
    "# At the end of each epoch, a decision tree classifier is trained using the data\n",
    "# from this epoch. The classifier is then used in the next epoch to choose actions.\n",
    "for epoch in range(epochs):\n",
    "  dat = []\n",
    "  lab = []\n",
    "  wins = 0\n",
    "  for _ in range(N):\n",
    "    done = False\n",
    "    observation = env.reset()\n",
    "    while not done:\n",
    "      # pick an action\n",
    "      action = actionpicker(clf, env, observation, epsilon)\n",
    "\n",
    "      dat += [observation + (action,)]\n",
    "      observation, reward, done, info = env.step(action)\n",
    "      if done:\n",
    "        target = reward\n",
    "      elif epoch == 0:\n",
    "        target = 0\n",
    "      else:\n",
    "        pred = clf.predict([observation + (0,), observation + (1,)])\n",
    "        target = max(pred)\n",
    "      lab += [target]\n",
    "      if reward > 0.0: wins += 1\n",
    "  clf = train(dat, lab)\n",
    "\n",
    "  env.close()\n",
    "#  print(dat)\n",
    "  print('Epoch %d: %d wins (%.2f%%)' % (epoch, wins, wins/N*100))\n",
    "  print('New classifier:')\n",
    "  print(tree.export_text(clf, class_names = [-1, 0, 1],\n",
    "                         feature_names = [\"holding\", \"dealer\", \"ace\", \"action\"]))\n",
    "  print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After repeated runs of the above code cell, the results always look like this:\n",
    "- Epoch 0 wins about 28% of the rounds.\n",
    "- Epoch 1 wins about 41% of the rounds.\n",
    "- Epoch 2 through 4 each wins about 37.5%.\n",
    "\n",
    "Interestingly, the best classifier is created from records of random actions (created at the end of epoch 0 and used in epoch 1). But even the best classifier has less than 50% wins. The code doesn't record the count of ties vs losses, so we don't know the actual win/loss amount in each round. value: [-0.51] value: [-0.58]value: [0.00]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classifier generated at the end of epoch 0 generally looks like:\n",
    "```\n",
    "|--- action <= 0.50\n",
    "|   |--- holding <= 18.50\n",
    "|   |   |--- value: [-0.38]\n",
    "|   |--- holding >  18.50\n",
    "|   |   |--- value: [0.57]\n",
    "|--- action >  0.50\n",
    "|   |--- holding <= 13.50\n",
    "|   |   |--- value: [-0.16]\n",
    "|   |--- holding >  13.50\n",
    "|   |   |--- ace <= 0.50\n",
    "|   |   |   |--- holding <= 17.50\n",
    "|   |   |   |   |--- value: [-0.57]\n",
    "|   |   |   |--- holding >  17.50\n",
    "|   |   |   |   |--- value: [-0.88]\n",
    "|   |   |--- ace >  0.50\n",
    "|   |   |   |--- value: [0.00]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classifier generated at the end of epoch 1 generally looks like:\n",
    "```\n",
    "|--- holding <= 18.50\n",
    "|   |--- holding <= 11.50\n",
    "|   |   |--- value: [-0.06]\n",
    "|   |--- holding >  11.50\n",
    "|   |   |--- holding <= 17.50\n",
    "|   |   |   |--- value: [-0.36]\n",
    "|   |   |--- holding >  17.50\n",
    "|   |   |   |--- value: [-0.05]\n",
    "|--- holding >  18.50\n",
    "|   |--- action <= 0.50\n",
    "|   |   |--- holding <= 19.50\n",
    "|   |   |   |--- value: [0.27]\n",
    "|   |   |--- holding >  19.50\n",
    "|   |   |   |--- value: [0.69]\n",
    "|   |--- action >  0.50\n",
    "|   |   |--- value: [-0.58]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classifier generated at the end of epoch 2 or later generally looks like:\n",
    "```\n",
    "|--- holding <= 18.50\n",
    "|   |--- dealer <= 7.50\n",
    "|   |   |--- dealer <= 1.50\n",
    "|   |   |   |--- value: [-0.71]\n",
    "|   |   |--- dealer >  1.50\n",
    "|   |   |   |--- value: [-0.20]\n",
    "|   |--- dealer >  7.50\n",
    "|   |   |--- value: [-0.50]\n",
    "|--- holding >  18.50\n",
    "|   |--- action <= 0.50\n",
    "|   |   |--- holding <= 19.50\n",
    "|   |   |   |--- value: [0.26]\n",
    "|   |   |--- holding >  19.50\n",
    "|   |   |   |--- value: [0.67]\n",
    "|   |--- action >  0.50\n",
    "|   |   |--- value: [-0.51]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each run of the code will likely generate slightly different values for each leaf node, but the splitting rules at each internal node are generally very consistent. These decision trees may look odd to human readers:\n",
    "- In the tree for epoch 0, the action splits at the root of the tree.\n",
    "- In the trees for epoch 1, 2, or later, the branch for holding <= 18.50 has no mention of action, so this branch is useless for making a decision on the action."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
