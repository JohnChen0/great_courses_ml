{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_ZnbwJGToM_l"
   },
   "source": [
    "In this notebook, we train a model to learn a strategy for Blackjack using a decision tree regression.\n",
    "\n",
    "Below we write an `actionpicker` function that decides what `action` to play given the `observation` from the Blackjack simulator, `env`.\n",
    "`epsilon` percent of the time, we decide on a random `action`.\n",
    "\n",
    "We also write a `train` function fits our decision tree regression to our `dat` and `labs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9nI21j7xoDA6"
   },
   "outputs": [],
   "source": [
    "def actionpicker(clf, env, observation, epsilon):\n",
    "  if clf == 0:\n",
    "    action = env.action_space.sample()\n",
    "  else:\n",
    "    pred = clf.predict([observation + (0,), observation + (1,)])\n",
    "    action = 1*(pred[1]>pred[0])\n",
    "  if random.random() < epsilon:\n",
    "    action = env.action_space.sample()\n",
    "  return(action)\n",
    "\n",
    "def train(dat, lab):\n",
    "  clf = tree.DecisionTreeRegressor(max_leaf_nodes = 6)\t\n",
    "  clf = clf.fit(dat, lab)\n",
    "  return(clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using Blackjack environment provided by gym API.\n",
    "It is a simplified Blackjack, where the player can only hit or stay at each turn.\n",
    "Other actions, such as split or double down, are not supported.\n",
    "Each card draw is from a fresh deck -- the API doesn't keep track of which cards have already been drawn.\n",
    "\n",
    "To create a new Blackjack environment:\n",
    "```\n",
    "env = gym.make(\"Blackjack-v0\", natural=False)\n",
    "```\n",
    "The optional parameter `natural` (which must be a keyword parameter instead of positional) determines whether 1.5 time reward is given to a user blackjack. By default no extra reward is given to blackjack.\n",
    "\n",
    "The Blackjack environment has three primary methods:\n",
    "- `env.reset() -> tuple[int, int, bool]` starts a new round, dealing two cards to the user and two cards to the dealer.\n",
    "  It returns a 3-tuple: total value of user's cards (with Ace valued at 11 if possible),\n",
    "  value of dealer's first card (dealer's second card is hidden at this point),\n",
    "  and whether the user has an Ace that is currently used as 11.\n",
    "  For the value of dealer's first card, Ace is represented by 1, regardless of whether it should be used as 11 or not.\n",
    "- `env.action_space.sample() -> int` randomly selects an action for the user.\n",
    "  It returns 0 (meaning stay) or 1 (meaning hit) with 50% probability each.\n",
    "- `env.step(action: int) -> tuple[tuple[int, int, bool], float, bool, dict]` takes one user action (either 0 for stay, or 1 for hit) and completes it.\n",
    "  Returns a 4-tuple. Item 0 is observation, a 3-tuple with the same meaning as that returned by `env.reset`;\n",
    "  item 1 is reward, amount won by the user; item 2 is done, indicating whether this round is done or not;\n",
    "  item 4 is an empty dict, probably for future use. `reward` is 0.0 if `done` is False.\n",
    "  - When `action` is 1, the code draws one card for the user. If the user is bust, the round completes and no action is possible until the env resets. Otherwise, the round is not complete, and the user can make another action.\n",
    "  - When `action` is 0, the code draws cards for dealer until the total value reaches at least 17, and the end round is complete.\n",
    "\n",
    "There are two attributes, `env.player` and `env.dealer`, that are useful for debugging. The return a list of all the cards currently held by the player and the dealer, respectively. Note that in a real game, the player can't see `env.dealer[1:]` while makeing a decision.\n",
    "\n",
    "The code below also calls `env.close()`, which actually does nothing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Li0Pfimp_eT"
   },
   "source": [
    "In the next code block, we train our Blackjack playing model and visualize the descision tree regression we have learned.  We will train our model 5 times, playing 100,000 rounds, and print the number of rounds it won each time.\n",
    "\n",
    "The code below has been modified slightly from the original code, to record more information. In addition to number of wins, it also records the number of ties and losses, number of natural blackjacks, and total reward (assuming natural pays 1.5). Inputs to the classifier are not changed (in particular, value for natural is still 1.0), so the classifier should behave exactly as the original code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WUzI-EbQX-xJ"
   },
   "outputs": [],
   "source": [
    "# action 0 is stay\n",
    "# action 1 is hit\n",
    "\n",
    "import gym\n",
    "from sklearn import tree\n",
    "import random\n",
    "\n",
    "env = gym.make(\"Blackjack-v0\")\n",
    "epochs = 5\n",
    "N = 100000\n",
    "epsilon = 0.1\n",
    "clf = 0\n",
    "\n",
    "# In epoch 0, the player always chooses random actions.\n",
    "# At the end of each epoch, a decision tree classifier is trained using the data\n",
    "# from this epoch. The classifier is then used in the next epoch to choose actions.\n",
    "for epoch in range(epochs):\n",
    "  dat = []\n",
    "  lab = []\n",
    "  wins = 0\n",
    "  ties = 0\n",
    "  losses = 0\n",
    "  naturals = 0\n",
    "  rewards = 0.0\n",
    "  for _ in range(N):\n",
    "    done = False\n",
    "    observation = env.reset()\n",
    "    while not done:\n",
    "      # pick an action\n",
    "      action = actionpicker(clf, env, observation, epsilon)\n",
    "\n",
    "      dat += [observation + (action,)]\n",
    "      observation, reward, done, info = env.step(action)\n",
    "      if done:\n",
    "        target = reward\n",
    "      elif epoch == 0:\n",
    "        target = 0\n",
    "      else:\n",
    "        pred = clf.predict([observation + (0,), observation + (1,)])\n",
    "        target = max(pred)\n",
    "      lab += [target]\n",
    "    rewards += reward\n",
    "    if reward > 0.0:\n",
    "      wins += 1\n",
    "      if gym.envs.toy_text.blackjack.is_natural(env.player):\n",
    "        naturals += 1\n",
    "        rewards += 0.5\n",
    "    elif reward == 0.0:\n",
    "      ties += 1\n",
    "    else:\n",
    "      losses += 1\n",
    "  clf = train(dat, lab)\n",
    "\n",
    "  env.close()\n",
    "#  print(dat)\n",
    "  assert wins + ties + losses == N\n",
    "  assert wins >= naturals\n",
    "  assert abs((wins + 0.5 * naturals - losses) - rewards) < 1e-5\n",
    "  print('Epoch %d: %d wins (%.2f%%) / %d ties (%.2f%%) / %d losses (%.2f%%) / %d naturals (%.2f%%) / reward %.1f' % (\n",
    "      epoch, wins, wins/N*100, ties, ties/N*100, losses, losses/N*100, naturals, naturals/N*100, rewards))\n",
    "  print('New classifier:')\n",
    "  print(tree.export_text(clf, class_names = [-1, 0, 1],\n",
    "                         feature_names = [\"holding\", \"dealer\", \"ace\", \"action\"]))\n",
    "  print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After repeated runs of the above code cell, the results always look like this:\n",
    "- Epoch 0 wins about 28% of the rounds, with average reward -0.39 per round.\n",
    "- Epoch 1 wins about 41% of the rounds, with average reward -0.08 per round.\n",
    "- Epoch 2 through 4 each wins about 37.5%, with average reward -0.18 per round.\n",
    "\n",
    "Interestingly, the best classifier is created from records of random actions (created at the end of epoch 0 and used in epoch 1). But even the best classifier has less than 50% wins. The code doesn't record the count of ties vs losses, so we don't know the actual win/loss amount in each round. value: [-0.51] value: [-0.58]value: [0.00]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classifier generated at the end of epoch 0 generally looks like:\n",
    "```\n",
    "|--- action <= 0.50\n",
    "|   |--- holding <= 18.50\n",
    "|   |   |--- value: [-0.38]\n",
    "|   |--- holding >  18.50\n",
    "|   |   |--- value: [0.57]\n",
    "|--- action >  0.50\n",
    "|   |--- holding <= 13.50\n",
    "|   |   |--- value: [-0.16]\n",
    "|   |--- holding >  13.50\n",
    "|   |   |--- ace <= 0.50\n",
    "|   |   |   |--- holding <= 17.50\n",
    "|   |   |   |   |--- value: [-0.57]\n",
    "|   |   |   |--- holding >  17.50\n",
    "|   |   |   |   |--- value: [-0.88]\n",
    "|   |   |--- ace >  0.50\n",
    "|   |   |   |--- value: [0.00]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classifier generated at the end of epoch 1 generally looks like:\n",
    "```\n",
    "|--- holding <= 18.50\n",
    "|   |--- holding <= 11.50\n",
    "|   |   |--- value: [-0.06]\n",
    "|   |--- holding >  11.50\n",
    "|   |   |--- holding <= 17.50\n",
    "|   |   |   |--- value: [-0.36]\n",
    "|   |   |--- holding >  17.50\n",
    "|   |   |   |--- value: [-0.05]\n",
    "|--- holding >  18.50\n",
    "|   |--- action <= 0.50\n",
    "|   |   |--- holding <= 19.50\n",
    "|   |   |   |--- value: [0.27]\n",
    "|   |   |--- holding >  19.50\n",
    "|   |   |   |--- value: [0.69]\n",
    "|   |--- action >  0.50\n",
    "|   |   |--- value: [-0.58]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classifier generated at the end of epoch 2 or later generally looks like:\n",
    "```\n",
    "|--- holding <= 18.50\n",
    "|   |--- dealer <= 7.50\n",
    "|   |   |--- dealer <= 1.50\n",
    "|   |   |   |--- value: [-0.71]\n",
    "|   |   |--- dealer >  1.50\n",
    "|   |   |   |--- value: [-0.20]\n",
    "|   |--- dealer >  7.50\n",
    "|   |   |--- value: [-0.50]\n",
    "|--- holding >  18.50\n",
    "|   |--- action <= 0.50\n",
    "|   |   |--- holding <= 19.50\n",
    "|   |   |   |--- value: [0.26]\n",
    "|   |   |--- holding >  19.50\n",
    "|   |   |   |--- value: [0.67]\n",
    "|   |--- action >  0.50\n",
    "|   |   |--- value: [-0.51]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each run of the code will likely generate slightly different values for each leaf node, but the splitting rules at each internal node are generally very consistent. These decision trees may look odd to human readers:\n",
    "- In the tree for epoch 0, the action splits at the root of the tree.\n",
    "- In the trees for epoch 1, 2, or later, the branch for holding <= 18.50 has no mention of action, so this branch is useless for making a decision on the action."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a first attempt to improve the code. It feeds correct rewards to the classfier when natural blackjack occurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WUzI-EbQX-xJ"
   },
   "outputs": [],
   "source": [
    "# action 0 is stay\n",
    "# action 1 is hit\n",
    "\n",
    "import gym\n",
    "from sklearn import tree\n",
    "import random\n",
    "\n",
    "env = gym.make(\"Blackjack-v0\", natural=True)\n",
    "epochs = 5\n",
    "N = 100000\n",
    "epsilon = 0.1\n",
    "clf = 0\n",
    "\n",
    "# In epoch 0, the player always chooses random actions.\n",
    "# At the end of each epoch, a decision tree classifier is trained using the data\n",
    "# from this epoch. The classifier is then used in the next epoch to choose actions.\n",
    "for epoch in range(epochs):\n",
    "  dat = []\n",
    "  lab = []\n",
    "  wins = 0\n",
    "  ties = 0\n",
    "  losses = 0\n",
    "  naturals = 0\n",
    "  rewards = 0.0\n",
    "  for _ in range(N):\n",
    "    done = False\n",
    "    observation = env.reset()\n",
    "    while not done:\n",
    "      # pick an action\n",
    "      action = actionpicker(clf, env, observation, epsilon)\n",
    "\n",
    "      dat += [observation + (action,)]\n",
    "      observation, reward, done, info = env.step(action)\n",
    "      if done:\n",
    "        target = reward\n",
    "      elif epoch == 0:\n",
    "        target = 0\n",
    "      else:\n",
    "        pred = clf.predict([observation + (0,), observation + (1,)])\n",
    "        target = max(pred)\n",
    "      lab += [target]\n",
    "    rewards += reward\n",
    "    if reward > 0.0:\n",
    "      wins += 1\n",
    "      if reward == 1.5:\n",
    "        naturals += 1\n",
    "    elif reward == 0.0:\n",
    "      ties += 1\n",
    "    else:\n",
    "      losses += 1\n",
    "  clf = train(dat, lab)\n",
    "\n",
    "  env.close()\n",
    "#  print(dat)\n",
    "  assert wins + ties + losses == N\n",
    "  assert wins >= naturals\n",
    "  assert abs((wins + 0.5 * naturals - losses) - rewards) < 1e-5\n",
    "  print('Epoch %d: %d wins (%.2f%%) / %d ties (%.2f%%) / %d losses (%.2f%%) / %d naturals (%.2f%%) / reward %.1f' % (\n",
    "      epoch, wins, wins/N*100, ties, ties/N*100, losses, losses/N*100, naturals, naturals/N*100, rewards))\n",
    "  print('New classifier:')\n",
    "  print(tree.export_text(clf, class_names = [-1, 0, 1],\n",
    "                         feature_names = [\"holding\", \"dealer\", \"ace\", \"action\"]))\n",
    "  print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result of the above code is no better than the original code.\n",
    "Examining the decision trees, one apparent problem is the classifier's main goal is to predict the reward,\n",
    "not the action that would lead to the best reward.\n",
    "This goal wastes lots of leaf nodes, so while we have 6 leaf nodes, the number of \"useful\" leaf nodes is much less.\n",
    "To compensate, we will increase the number of leaf nodes to 16."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dat, lab):\n",
    "  clf = tree.DecisionTreeRegressor(max_leaf_nodes = 16)\t\n",
    "  clf = clf.fit(dat, lab)\n",
    "  return(clf)\n",
    "\n",
    "env = gym.make(\"Blackjack-v0\", natural=True)\n",
    "epochs = 5\n",
    "N = 100000\n",
    "epsilon = 0.1\n",
    "clf = 0\n",
    "\n",
    "# In epoch 0, the player always chooses random actions.\n",
    "# At the end of each epoch, a decision tree classifier is trained using the data\n",
    "# from this epoch. The classifier is then used in the next epoch to choose actions.\n",
    "for epoch in range(epochs):\n",
    "  dat = []\n",
    "  lab = []\n",
    "  wins = 0\n",
    "  ties = 0\n",
    "  losses = 0\n",
    "  naturals = 0\n",
    "  rewards = 0.0\n",
    "  for _ in range(N):\n",
    "    done = False\n",
    "    observation = env.reset()\n",
    "    while not done:\n",
    "      # pick an action\n",
    "      action = actionpicker(clf, env, observation, epsilon)\n",
    "\n",
    "      dat += [observation + (action,)]\n",
    "      observation, reward, done, info = env.step(action)\n",
    "      if done:\n",
    "        target = reward\n",
    "      elif epoch == 0:\n",
    "        target = 0\n",
    "      else:\n",
    "        pred = clf.predict([observation + (0,), observation + (1,)])\n",
    "        target = max(pred)\n",
    "      lab += [target]\n",
    "    rewards += reward\n",
    "    if reward > 0.0:\n",
    "      wins += 1\n",
    "      if reward == 1.5:\n",
    "        naturals += 1\n",
    "    elif reward == 0.0:\n",
    "      ties += 1\n",
    "    else:\n",
    "      losses += 1\n",
    "  clf = train(dat, lab)\n",
    "\n",
    "  env.close()\n",
    "#  print(dat)\n",
    "  assert wins + ties + losses == N\n",
    "  assert wins >= naturals\n",
    "  assert abs((wins + 0.5 * naturals - losses) - rewards) < 1e-5\n",
    "  print('Epoch %d: %d wins (%.2f%%) / %d ties (%.2f%%) / %d losses (%.2f%%) / %d naturals (%.2f%%) / reward %.1f' % (\n",
    "      epoch, wins, wins/N*100, ties, ties/N*100, losses, losses/N*100, naturals, naturals/N*100, rewards))\n",
    "  print('New classifier:')\n",
    "  print(tree.export_text(clf, class_names = [-1, 0, 1],\n",
    "                         feature_names = [\"holding\", \"dealer\", \"ace\", \"action\"]))\n",
    "  print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result appears to be slightly better, at least for epoch 1, which now has 41.5% wins, with reward -0.07 per game.\n",
    "The other epochs appear to be the same as before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell is my own code.\n",
    "It plays 100,000 rounds of random games,\n",
    "and simply records the rewards for each action under each observation.\n",
    "Then it always plays the action with higher average reward for any observation it encounters.\n",
    "\n",
    "This achieves the best results so far: 43% wins (including 4% naturals), 9% ties, and 48% losses.\n",
    "The average reward per round is -0.03."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "def mean(count, total):\n",
    "  if count == 0:\n",
    "    return 0.0\n",
    "  else:\n",
    "    return total / count\n",
    "\n",
    "env = gym.make(\"Blackjack-v0\", natural=True)\n",
    "N = 100000\n",
    "\n",
    "# Training by playing random games.\n",
    "stats = {}\n",
    "for _ in range(N):\n",
    "  done = False\n",
    "  observation = env.reset()\n",
    "  while not done:\n",
    "    # Choose an action. Try to balance number of stays and hits for a given observation.\n",
    "    stat = stats.setdefault(observation, [[0, 0.0], [0, 0.0]])\n",
    "    if stat[0][0] > stat[1][0]:\n",
    "      action = 1\n",
    "    elif stat[0][0] < stat[1][0]:\n",
    "      action = 0\n",
    "    else:\n",
    "      action = env.action_space.sample()\n",
    "\n",
    "    # Play the action, and record the reward. If round is not done yet, try to\n",
    "    # estimate the reward using preview encounters of the new observation.\n",
    "    observation, reward, done, info = env.step(action)\n",
    "    if not done:\n",
    "      new_stat = stats.get(observation)\n",
    "      if new_stat:\n",
    "        reward = max(mean(*new_stat[0]), mean(*new_stat[1]))\n",
    "    stat[action][0] += 1\n",
    "    stat[action][1] += reward\n",
    "\n",
    "actions = {}\n",
    "for observation, stat in stats.items():\n",
    "  actions[observation] = 1 if mean(*stat[0]) < mean(*stat[1]) else 0\n",
    "\n",
    "# Play the game for real.\n",
    "wins = 0\n",
    "ties = 0\n",
    "losses = 0\n",
    "naturals = 0\n",
    "rewards = 0.0\n",
    "for _ in range(N):\n",
    "  done = False\n",
    "  observation = env.reset()\n",
    "  while not done:\n",
    "    action = actions.get(observation, 0)\n",
    "    observation, reward, done, info = env.step(action)\n",
    "  rewards += reward\n",
    "  if reward > 0.0:\n",
    "    wins += 1\n",
    "    if reward == 1.5:\n",
    "      naturals += 1\n",
    "  elif reward == 0.0:\n",
    "      ties += 1\n",
    "  else:\n",
    "    losses += 1\n",
    "\n",
    "assert wins + ties + losses == N\n",
    "assert wins >= naturals\n",
    "assert abs((wins + 0.5 * naturals - losses) - rewards) < 1e-5\n",
    "print('%d wins (%.2f%%) / %d ties (%.2f%%) / %d losses (%.2f%%) / %d naturals (%.2f%%) / reward %.1f' % (\n",
    "  wins, wins/N*100, ties, ties/N*100, losses, losses/N*100, naturals, naturals/N*100, rewards))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
