{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u-O-IaFDLM3u"
   },
   "source": [
    "In this notebook we will build a speech recognition model.  \n",
    "\n",
    "Below we'll import the libraries we'll be using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2727,
     "status": "ok",
     "timestamp": 1596142272224,
     "user": {
      "displayName": "Michael Littman",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh1OI1zp1NevMSZ87UN47k8bTZ--Ijr4v3ORV33Vg=s64",
      "userId": "06751891446459829367"
     },
     "user_tz": 240
    },
    "id": "wrWNC6CiH7Dd"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import librosa   #for audio processing\n",
    "import IPython.display as ipd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VuKhes1QLe3b"
   },
   "source": [
    "Next, we'll download the dataset of speech commands from tensorflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 17694,
     "status": "ok",
     "timestamp": 1596142287202,
     "user": {
      "displayName": "Michael Littman",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh1OI1zp1NevMSZ87UN47k8bTZ--Ijr4v3ORV33Vg=s64",
      "userId": "06751891446459829367"
     },
     "user_tz": 240
    },
    "id": "LUrISUQjkscm",
    "outputId": "46d974c1-eca8-4457-c8a5-6432e9a69a07"
   },
   "outputs": [],
   "source": [
    "if not os.path.exists('speech_commands_v0.01.tar.gz'):\n",
    "    import urllib.request\n",
    "    data = urllib.request.urlopen('http://download.tensorflow.org/data/speech_commands_v0.01.tar.gz').read()\n",
    "    with open('speech_commands_v0.01.tar.gz', 'wb') as f:\n",
    "        f.write(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JZVW2lmaLkHz"
   },
   "source": [
    "Here, we unzip the file we downloaded from tensorflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 58040,
     "status": "ok",
     "timestamp": 1596142331149,
     "user": {
      "displayName": "Michael Littman",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh1OI1zp1NevMSZ87UN47k8bTZ--Ijr4v3ORV33Vg=s64",
      "userId": "06751891446459829367"
     },
     "user_tz": 240
    },
    "id": "52b5XAo0k1lT"
   },
   "outputs": [],
   "source": [
    "if not os.path.exists('speech_commands'):\n",
    "    import tarfile\n",
    "    os.mkdir('speech_commands')\n",
    "    tarfile.open('speech_commands_v0.01.tar.gz').extractall('speech_commands')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data set contains sound clips of the following 30 English words:\n",
    "* zero, one, two, three, four, five, six, seven, eight, nine\n",
    "* left, right, up, down\n",
    "* yes, no, on, off, stop, go\n",
    "* bed, bird, cat, dog, house, tree\n",
    "* marvin, sheila, happy, wow\n",
    "\n",
    "Each word has about 2000 clips, stored as .wav files.\n",
    "There are six additional clips in `_background_noise_` directory.\n",
    "Most clips are 1 second long, though some clips have slightly different durations.\n",
    "All clips have sampling rates of 16,000 samples per second."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating some statistics of the data set.\n",
    "The output of the next code cell is:\n",
    "```\n",
    "bed    1713 samples (1484 one-second samples)\r\n",
    "bird   1731 samples (1521 one-second samples)\r\n",
    "cat    1733 samples (1515 one-second samples)\r\n",
    "dog    1746 samples (1547 one-second samples)\r\n",
    "down   2359 samples (2152 one-second samples)\r\n",
    "eight  2352 samples (2111 one-second samples)\r\n",
    "five   2357 samples (2161 one-second samples)\r\n",
    "four   2372 samples (2158 one-second samples)\r\n",
    "go     2372 samples (2101 one-second samples)\r\n",
    "happy  1742 samples (1549 one-second samples)\r\n",
    "house  1750 samples (1560 one-second samples)\r\n",
    "left   2353 samples (2165 one-second samples)\r\n",
    "marvin 1746 samples (1578 one-second samples)\r\n",
    "nine   2364 samples (2174 one-second samples)\r\n",
    "no     2375 samples (2098 one-second samples)\r\n",
    "off    2357 samples (2143 one-second samples)\r\n",
    "on     2367 samples (2105 one-second samples)\r\n",
    "one    2370 samples (2103 one-second samples)\r\n",
    "right  2367 samples (2155 one-second samples)\r\n",
    "seven  2377 samples (2170 one-second samples)\r\n",
    "sheila 1734 samples (1578 one-second samples)\r\n",
    "six    2369 samples (2199 one-second samples)\r\n",
    "stop   2380 samples (2174 one-second samples)\r\n",
    "three  2356 samples (2143 one-second samples)\r\n",
    "tree   1733 samples (1521 one-second samples)\r\n",
    "two    2373 samples (2140 one-second samples)\r\n",
    "up     2375 samples (2062 one-second samples)\r\n",
    "wow    1745 samples (1525 one-second samples)\r\n",
    "yes    2377 samples (2157 one-second samples)\r\n",
    "zero   2376 samples (2203 one-second samples)\r\n",
    "_background_noise_    6 samples (   0 one-second samples)\r\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top = 'speech_commands'\n",
    "for word in os.listdir(top):\n",
    "  word_path = os.path.join(top, word)\n",
    "  if not os.path.isdir(word_path):\n",
    "    continue\n",
    "  total = one_second = 0\n",
    "  for file in os.listdir(word_path):\n",
    "    if not file.endswith('.wav'):\n",
    "      continue\n",
    "    file_path = os.path.join(word_path, file)\n",
    "    samples, sample_rate = librosa.load(file_path, sr=None)\n",
    "    total += 1\n",
    "    if sample_rate != 16000:\n",
    "      print(f'{file_path} has wrong sample rate {sample_rate}')\n",
    "    elif samples.shape == (16000,):\n",
    "      one_second += 1\n",
    "  print('%-6s %4d samples (%4d one-second samples)' % (word, total, one_second))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eI-bgCqOLoEA"
   },
   "source": [
    "Let's plot the waveform of an example spoken command, `samples`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 293
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 62373,
     "status": "ok",
     "timestamp": 1596142338747,
     "user": {
      "displayName": "Michael Littman",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh1OI1zp1NevMSZ87UN47k8bTZ--Ijr4v3ORV33Vg=s64",
      "userId": "06751891446459829367"
     },
     "user_tz": 240
    },
    "id": "DVUj3Z5cI4Vm",
    "outputId": "b12a98d8-1a32-4101-ab29-6477a2d407c4"
   },
   "outputs": [],
   "source": [
    "train_path = 'speech_commands/'\n",
    "filename = train_path+'no/afe0b87d_nohash_0.wav'\n",
    "# By specifying sr=None, librosa.load keeps the original sampling rate of the clip,\n",
    "# which is 16,000 samples per second for all clips in this data set.\n",
    "# If an int is given (e.g., sr=20000), the clip will be resampled to that rate.\n",
    "# If the sr parameter is not given, it defaults to sr=22050.\n",
    "# samples will be a one-dimensional np.ndarray of type float32,\n",
    "# with its size equal to the number of samples.\n",
    "# (This particular clips is one second, so samples.shape is (16000,).)\n",
    "# Each element of the array is between -1.0 and 0.9999695 (i.e., 32767/32768).\n",
    "# (The original data are 2-byte integers between -32768 and 32767,\n",
    "# and they were scaled by dividing with 32768.\n",
    "samples, sample_rate = librosa.load(filename, sr = 16000)\n",
    "fig = plt.figure(figsize=(14, 8))\n",
    "ax1 = fig.add_subplot(211)\n",
    "ax1.set_title('Raw signal of ' + filename)\n",
    "ax1.set_xlabel('time')\n",
    "ax1.set_ylabel('Amplitude')\n",
    "ax1.plot(np.linspace(0, len(samples)/sample_rate, len(samples)), samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rODDxUb3Lx_O"
   },
   "source": [
    "Below we will play the the `samples` audio command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 75
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 53453,
     "status": "ok",
     "timestamp": 1596142338748,
     "user": {
      "displayName": "Michael Littman",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh1OI1zp1NevMSZ87UN47k8bTZ--Ijr4v3ORV33Vg=s64",
      "userId": "06751891446459829367"
     },
     "user_tz": 240
    },
    "id": "RURz8GG2vPHr",
    "outputId": "2e66f477-4ddb-43f9-91f3-30fc3578e83f"
   },
   "outputs": [],
   "source": [
    "ipd.Audio(samples,rate=sample_rate,autoplay=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4Ve2s44HL6G5"
   },
   "source": [
    "Below we load the data into `all_wavs` and their respective labels into `all_labs`.  The labels are either `yes` or `no`.\n",
    "\n",
    "We'll also print the number of examples in `all_wavs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 440915,
     "status": "ok",
     "timestamp": 1593903868705,
     "user": {
      "displayName": "Michael Littman",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh1OI1zp1NevMSZ87UN47k8bTZ--Ijr4v3ORV33Vg=s64",
      "userId": "06751891446459829367"
     },
     "user_tz": 240
    },
    "id": "W--i8r_iwMv-",
    "outputId": "c86b574f-84d4-431a-ddd5-2b24b46cc0ab"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "directory = 'speech_commands/'\n",
    "\n",
    "all_wavs = []\n",
    "all_labs = []\n",
    "for label in ['yes', 'no']:\n",
    "    print(label)\n",
    "    wavs = [f for f in os.listdir(directory + label) if f.endswith('.wav')]\n",
    "    for wav in wavs:\n",
    "        samples, sample_rate = librosa.load(directory + label + '/' + wav, sr=None)\n",
    "        if len(samples) == 16000: \n",
    "            all_wavs.append(samples)\n",
    "            all_labs.append(label)\n",
    "print(\n",
    "    len(all_wavs), 'examples,',\n",
    "    all_labs.count('yes'), 'yes,', all_labs.count('no'), 'no.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5OS6-Zk1NwJd"
   },
   "source": [
    "Below we split our training and test data.  `X_train` is our processed audio files for training and `y_train` are their labels.  `X_test` and `y_test` are our test audio files and their labels, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Mi2-T32hwb6T"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    " \n",
    "all_wavs = np.array(all_wavs).reshape(-1,16000,1)\n",
    "all_labs = np.array([lab == 'yes' for lab in all_labs])\n",
    "X_train, X_test, y_train, y_test = train_test_split(all_wavs,all_labs,test_size = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qyYmvoBbOcMv"
   },
   "source": [
    "In the following lines, we will build together the layers of our model for speech recognition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 15330,
     "status": "ok",
     "timestamp": 1593904125706,
     "user": {
      "displayName": "Michael Littman",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh1OI1zp1NevMSZ87UN47k8bTZ--Ijr4v3ORV33Vg=s64",
      "userId": "06751891446459829367"
     },
     "user_tz": 240
    },
    "id": "_gLlhz_vy3V6",
    "outputId": "71a4423e-1104-4193-b324-772dfca87cfe"
   },
   "outputs": [],
   "source": [
    "from keras.layers import Conv1D, Input, MaxPooling1D, Flatten, Dense\n",
    "from keras.models import Model\n",
    " \n",
    "inputs = Input(shape=(16000,1))\n",
    " \n",
    "#First Conv1D layer\n",
    "conv = Conv1D(8,13, padding='valid', activation='relu', strides=1)(inputs)\n",
    "conv = MaxPooling1D(3)(conv)\n",
    " \n",
    "#Second Conv1D layer\n",
    "conv = Conv1D(16, 11, padding='valid', activation='relu', strides=1)(conv)\n",
    "conv = MaxPooling1D(3)(conv)\n",
    " \n",
    "#Third Conv1D layer\n",
    "conv = Conv1D(32, 9, padding='valid', activation='relu', strides=1)(conv)\n",
    "conv = MaxPooling1D(3)(conv)\n",
    " \n",
    "#Fourth Conv1D layer\n",
    "conv = Conv1D(64, 7, padding='valid', activation='relu', strides=1)(conv)\n",
    "conv = MaxPooling1D(3)(conv)\n",
    " \n",
    "#Flatten layer\n",
    "conv = Flatten()(conv)\n",
    " \n",
    "#Dense Layer 1\n",
    "conv = Dense(256, activation='relu')(conv)\n",
    " \n",
    "#Dense Layer 2\n",
    "conv = Dense(128, activation='relu')(conv)\n",
    " \n",
    "outputs = Dense(1, activation='sigmoid')(conv)\n",
    " \n",
    "model = Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IuVQFr34Oj2y"
   },
   "source": [
    "We then `fit` the model.  We use a `mean_squared_error` `loss` and optimize the weigths using use `adam` as our `optimizer`. We iterate of the data 15 times.  Each time, or `epoch`, we print out the `accuracy` and `loss` of our model so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 544
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 38824,
     "status": "ok",
     "timestamp": 1593904155610,
     "user": {
      "displayName": "Michael Littman",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh1OI1zp1NevMSZ87UN47k8bTZ--Ijr4v3ORV33Vg=s64",
      "userId": "06751891446459829367"
     },
     "user_tz": 240
    },
    "id": "oTCjBNo9y8XT",
    "outputId": "5eea60a0-f7a4-46a0-9ebd-046f409f9e4f"
   },
   "outputs": [],
   "source": [
    "model.compile(loss='mean_squared_error',optimizer='adam',metrics=['accuracy'])\n",
    " \n",
    "model.fit(X_train, y_train ,epochs=15, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l7IGY2JePP5B"
   },
   "source": [
    "We then report the final `accuracy` and `loss` on the `X_test` and `y_test` data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2084,
     "status": "ok",
     "timestamp": 1593904700475,
     "user": {
      "displayName": "Michael Littman",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh1OI1zp1NevMSZ87UN47k8bTZ--Ijr4v3ORV33Vg=s64",
      "userId": "06751891446459829367"
     },
     "user_tz": 240
    },
    "id": "zbnF7hTXy_zr",
    "outputId": "cac1f552-df93-4fcc-e1e8-2fdd98f0c6e1"
   },
   "outputs": [],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1223,
     "status": "ok",
     "timestamp": 1593904816895,
     "user": {
      "displayName": "Michael Littman",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh1OI1zp1NevMSZ87UN47k8bTZ--Ijr4v3ORV33Vg=s64",
      "userId": "06751891446459829367"
     },
     "user_tz": 240
    },
    "id": "4TyyipuYIUPP",
    "outputId": "35804b5f-5fe3-4c0d-a0c2-ad73f9321655"
   },
   "outputs": [],
   "source": [
    "# We have a total of 4255 samples.\n",
    "# The training set has 4255 * 80% = 3404 samples.\n",
    "# The test set has 4255 * 20% = 851 samples.\n",
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "L20.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
