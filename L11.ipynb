{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oJVOsD5TDbPK"
   },
   "source": [
    "In this notebook, we will write our own k-means model from scratch and use it to cluster handwritten numbers from the MNIST dataset\n",
    "\n",
    "\n",
    "In the cell below, we create an `assign_data` function, which takes the `data` and the `centers` for each cluster and makes an assignment of each datapoint in `data` to the closest of the `centers`, `centerids`.  We extract `n`, the number of datapoints, `d`, the dimensionality of the datapoints, and `k` the number of centers.\n",
    "\n",
    "Next, we need to compute the squared distance between each center and each data point.\n",
    "\n",
    "Reshaping the data to be 1 x `n` x `d`, and the centers to be `k` x 1 x `d` signals to numpy that when it subtracts these two arrays, it creates an array of shape `k` x `n` x `d`. That is, it computes all combinations of the `k` centers and the `n` datapoints for each of the `d` dimensions. We assign those differences to `res`.\n",
    "\n",
    "Squaring each of the differences, then summing along dimension 2 --- that’s the components of the vectors --- produces the sum of squared distances, which is the squared distance between the centers and the datapoints.  The resulting array is of shape `k` x `n`.\n",
    "\n",
    "`assign_data` also computes the `loss`, which the sum of the squared differences. We want to know which center has the *smallest* squared distance for each data point. `argmin` produces the index of an array with the smallest value along the given dimension. Here, we’re using dimension 0, which varies over the `k` centers. `centerids` is now an array with one integer for each datapoint that indicates which of the centers is closest.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QtZgh0QsJpid"
   },
   "outputs": [],
   "source": [
    "def assign_data(data,centers):\n",
    "  # n is the number of data points\n",
    "  n = len(data)\n",
    "  # d is the dimensionality of the data points\n",
    "  d = len(data[0])\n",
    "  # k is the number of clusters\n",
    "  k = len(centers)\n",
    "  # first, subtract the set of centers from each data point\n",
    "  res = np.reshape(data,(1,n,d))-np.reshape(centers,(k,1,d))\n",
    "  # sum the squared differences\n",
    "  res2 = np.add.reduce(res**2,2)\n",
    "  # assign each data point to its closest center\n",
    "  centerids = np.apply_along_axis(np.argmin,0,res2)\n",
    "  # While we're here, make a note of the loss\n",
    "  loss = sum(np.apply_along_axis(np.min,0,res2))\n",
    "  return(centerids, loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2x4tp5vkDqDh"
   },
   "source": [
    "Next we'll compute the mean of each of the `k` centers using the `data` and their `centerids` assignments. `compute_means` takes the data and the center ids and computes the centers by averaging all of the datapoints with the same id.  This will be used to update the `centers`. \n",
    "\n",
    "After extracting the number of datapoints and dimension, we initialize the array of center locations to a `k` x `d` array of all zeros.\n",
    "\n",
    "For each of the cluster id values from 0 to `k`, we do the following operations:\n",
    " - First, form a smaller array, `cols`, consisting of all the datapoints with the current center id.\n",
    " - To be robust, we make sure `cols` has a length greater than zero. That can happen if there’s a center that has been elbowed out of the running by the other centers being closer to all of the data points. \n",
    "  - If it equals zero, that means our center is out of the action and we should probably pick a different location for it. We simply choose one of the data points at random to be this new location.\n",
    "\n",
    "- We want to move the center to the `mean` of the closest points. Numpy’s `mean` method computes the average of an array along any given dimension. Here, we choose dimension 0, which corresponds to the different data points. `mean` produces a component-wise average of all the data points with cluster id equal to `i`.\n",
    "- After completing the loop, we return the newly computed `centers`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vifay3EeJqi-"
   },
   "outputs": [],
   "source": [
    "def compute_means(data, centerids, k):\n",
    "  # n is number of data points\n",
    "  n = len(data)\n",
    "  # d is dimensionality of the data points\n",
    "  d = len(data[0])\n",
    " \n",
    "  # Zero out the centers\n",
    "  centers = np.zeros(shape=(k,d))\n",
    " \n",
    "  # loop over the clusters\n",
    "  for i in range(k):\n",
    "    # Gather the data points assigned to cluster i\n",
    "    cols = np.array([data[j] for j in range(n) if centerids[j] == i])\n",
    "    # Average to get mean for that cluster\n",
    "    if len(cols) == 0: \n",
    "      centers[i] = data[random.randint(0,n-1)]\n",
    "    else:\n",
    "      centers[i] = cols.mean(0)\n",
    "  return(centers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5-qbkig5D29K"
   },
   "source": [
    "With these two functions, we can build a `kmeans` model, which takes in the `data` and number of clusters, `k` and iteratively builds `k` clusters and updates them relative to the `loss`.  \n",
    "\n",
    "We initialize the `k` centers by selecting random data points. We loop until the `loss` stops changing. If `oldloss` is different from the new `loss`, we use `assign_data` to assign each datapoint to its closest center. Then, we use `compute_means` to move the centers to the means of the points assigned to them. We repeat until the `loss` stops changing, returning the final `loss` and `centers`.  \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ex_bJQGsJql8"
   },
   "outputs": [],
   "source": [
    "def kmeans(data, k):\n",
    "  n = len(data)\n",
    "  d = len(data[0])\n",
    "  # grab the centers from random points\n",
    "  centers = data[[random.randint(0,n-1) for i in range(k)]]\n",
    "  oldloss = 0\n",
    "  loss = 1\n",
    "  while oldloss != loss:\n",
    "    oldloss = loss\n",
    "    centerids, loss = assign_data(data,centers)\n",
    "    centers = compute_means(data, centerids, k)\n",
    "  return(centers, loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zxwnWjHWD7Cd"
   },
   "source": [
    "We will download the MNIST dataset and split the data into training data, `X_train` and `y_train` and test data, `X_test` and `y_test`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 25263,
     "status": "ok",
     "timestamp": 1590502090548,
     "user": {
      "displayName": "JZ Forde",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiwSogjDlg0BxeyyCUFJ3t-a6Mt210NSXjrBuLMYw=s64",
      "userId": "11263008094098858557"
     },
     "user_tz": 240
    },
    "id": "DZohJqLrJqoS",
    "outputId": "9f8c8492-7d3d-4c15-b6eb-9c5e3309624e"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "data = fetch_openml(name='mnist_784')\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_test, y_test, test_size=0.33)\n",
    "len(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NsiSIN2rELNf"
   },
   "source": [
    "Here, we run `kmeans` on our `X_train` data where `k=10`.  We run `kmeans` 9 times and record the `bestcenters` which have the `bestloss` among the recorded losses.  We then find the accuracy of these new centers on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 120302,
     "status": "ok",
     "timestamp": 1590502210859,
     "user": {
      "displayName": "JZ Forde",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiwSogjDlg0BxeyyCUFJ3t-a6Mt210NSXjrBuLMYw=s64",
      "userId": "11263008094098858557"
     },
     "user_tz": 240
    },
    "id": "85T_3_oHKa31",
    "outputId": "ef60f19f-5c61-4e91-a656-87d512766ab8"
   },
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "import math\n",
    "from functools import reduce\n",
    "import random\n",
    "\n",
    "#for nlabeled in range(20,len(X_train),10):\n",
    "nlabeled = 20\n",
    "if True:\n",
    "  print(nlabeled)\n",
    "  ans = []\n",
    "  k = 10 # 2 # 5 # 20\n",
    "  if True:\n",
    "    bestcenters, bestloss = kmeans(X_train, k)\n",
    "    for rep in range(9):\n",
    "      centers, loss = kmeans(X_train, k)\n",
    "      if loss < bestloss: bestcenters, bestloss = centers, loss\n",
    "    # How do we test the clustering that was discovered?\n",
    "    # Assign testing points to clusters\n",
    "    test_centerids, loss = assign_data(X_test, bestcenters)\n",
    "\n",
    "    # Use the labeled examples to label the clusters\n",
    "    train_centerids, loss = assign_data(X_train[:nlabeled], bestcenters)\n",
    "    #print(train_centerids)\n",
    "    #print(y_train[:nlabeled])\n",
    "    labs = y_train[:nlabeled]\n",
    "\n",
    "#    clust_labs = np.zeros(shape=(k))\n",
    "    clust_labs = np.repeat(labs[0],k)\n",
    "    for i in range(k):\n",
    "      mode = stats.mode(labs[train_centerids == i]).mode\n",
    "      if len(mode) > 0: clust_labs[i] = mode[0]\n",
    "\n",
    "# print(clust_labs)\n",
    "    ans = ans + [(k,sum(clust_labs[test_centerids] == y_test)/len(y_test))]\n",
    "#    plt.plot(X_test[clust_labs[test_centerids] == 0,0],X_test[clust_labs[test_centerids] == 0,1],'o',color='r')\n",
    "#    plt.plot(X_test[clust_labs[test_centerids] == 1,0],X_test[clust_labs[test_centerids] == 1,1],'o',color='b')\n",
    "#    plt.show()\n",
    "\n",
    "#  print(ans)\n",
    "  print(reduce((lambda x, y: x if x[1] > y[1] else y), ans))\n",
    "  labids, loss = assign_data(X_test, X_train[:nlabeled])\n",
    "  print(nlabeled, sum(y_train[labids] == y_test)/len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3iz6oNKK8Tux"
   },
   "source": [
    "We'll next print the images that best represent the centers of each of our clusters in K-means and the label for each of the clusters\n",
    "\n",
    "We will also calcuate the percent accuracy of the clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 620
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5964,
     "status": "ok",
     "timestamp": 1590502234267,
     "user": {
      "displayName": "JZ Forde",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiwSogjDlg0BxeyyCUFJ3t-a6Mt210NSXjrBuLMYw=s64",
      "userId": "11263008094098858557"
     },
     "user_tz": 240
    },
    "id": "er8dfhHEJqrA",
    "outputId": "2b6009cb-d25e-4349-c162-91c23b76443d"
   },
   "outputs": [],
   "source": [
    "!pip install keras=='2.3.1'\n",
    "from keras.preprocessing.image import array_to_img\n",
    "\n",
    "train_centerids, loss = assign_data(X_train, bestcenters)\n",
    "test_centerids, loss = assign_data(X_test, bestcenters)\n",
    "\n",
    "clust_labs = np.repeat(labs[0],k)\n",
    "for i in range(len(bestcenters)):\n",
    "  display(array_to_img(np.reshape(bestcenters[i],(28,28,1)), scale=False))\n",
    "  clust_labs[i] = y_train[train_centerids == i][0]\n",
    "  print(clust_labs[i])\n",
    "#  mode = stats.mode(y_train[train_centerids == i]).mode\n",
    "#  print(mode[0])\n",
    "#  if len(mode) > 0: clust_labs[i] = mode[0]\n",
    "\n",
    "sum(clust_labs[test_centerids] == y_test)/len(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oZjP6jnBgOxB"
   },
   "source": [
    "Finally, we'll rewrite the K-means model as an active learning problem and perform semi-supervised clustering of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 694670,
     "status": "ok",
     "timestamp": 1590499458205,
     "user": {
      "displayName": "JZ Forde",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiwSogjDlg0BxeyyCUFJ3t-a6Mt210NSXjrBuLMYw=s64",
      "userId": "11263008094098858557"
     },
     "user_tz": 240
    },
    "id": "Uzl-FUtPHMEx",
    "outputId": "077b7d99-8007-4d2e-cd83-c946dbf32c63"
   },
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "import math\n",
    "from functools import reduce\n",
    "\n",
    "# ACTIVE LEARNING VERSION\n",
    "\n",
    "#for nlabeled in range(20,len(X_train),10):\n",
    "nlabeled = 10\n",
    "if True:\n",
    "  ans = []\n",
    "  k = 50 # 10 # 2 # 5 # 20\n",
    "  if True:\n",
    "#  for k in range(10,200,50):\n",
    "    bestcenters, bestloss = kmeans(X_train, k)\n",
    "    for rep in range(9):\n",
    "      centers, loss = kmeans(X_train, k)\n",
    "      if loss < bestloss: bestcenters, bestloss = centers, loss\n",
    "    # How do we test the clustering that was discovered?\n",
    "    # Assign testing points to clusters\n",
    "    test_centerids, loss = assign_data(X_test, bestcenters)\n",
    "\n",
    "    # Let's label one example in each category\n",
    "    train_centerids, loss = assign_data(X_train, bestcenters)\n",
    "\n",
    "    clust_labs = np.repeat(labs[0],k)\n",
    "    for i in range(len(bestcenters)):\n",
    "      clust_labs[i] = y_train[train_centerids == i][0]\n",
    "\n",
    "    # semi-supervised clustering\n",
    "    print(k,sum(clust_labs[test_centerids] == y_test)/len(y_test))\n",
    "\n",
    "    # nearest neighbors\n",
    "    labids, loss = assign_data(X_test, X_train[:k])\n",
    "    print(k, sum(y_train[labids] == y_test)/len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Yk8rAi6ATrIL"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "L11.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
