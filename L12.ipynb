{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bFSFDwOuJHBW"
   },
   "source": [
    "In this notebook, we will build a recomendation system for academic papers. \n",
    "\n",
    "Below, we download the data we'll be using from GitHub. `vocab2.txt` contains the words in the tiles of the academic papers, and `cb.txt` contains the titles of the papers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 561
    },
    "executionInfo": {
     "elapsed": 6331,
     "status": "ok",
     "timestamp": 1593607843836,
     "user": {
      "displayName": "Michael Littman",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh1OI1zp1NevMSZ87UN47k8bTZ--Ijr4v3ORV33Vg=s64",
      "userId": "06751891446459829367"
     },
     "user_tz": 240
    },
    "id": "2tJcRhHcQYUy",
    "outputId": "748b8287-7e66-4852-db67-da8cd8546d57"
   },
   "outputs": [],
   "source": [
    "!wget https://github.com/mlittmancs/great_courses_ml/raw/master/data/vocab2.txt\n",
    "!wget https://github.com/mlittmancs/great_courses_ml/raw/master/data/cb.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RwCUD_TANklv"
   },
   "source": [
    "Here we write a few functions that will help us in processing the data:\n",
    "\n",
    "- `readvocab` creates a `vocab_dict` with a count of the number of times a word occurs in our paper titles\n",
    "- `tokenize` turns each set of words in a title, `string`, into a count of the number of times each word in the title occurs\n",
    "- `getdat`, which takes the titles and returns a list of titles with their word counts, `dat`, and a list of labels indicating if the user found the title interesting, `labs`\n",
    "\n",
    "We will use our function to process our data to get our vectorized titles, `dat`, and their labels `labs`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V-Fu_cwlZGIU"
   },
   "outputs": [],
   "source": [
    "# read in the vocabulary file \n",
    "def readvocab():\n",
    "   # keep track of the number of words\n",
    "    lexiconsize = 0\n",
    "   # initialize an empty dictionary\n",
    "    vocab_dict = {}\n",
    "   # create a catch-all feature (vector component) for all unknown words\n",
    "    vocab_dict[\"@unk\"] = lexiconsize\n",
    "    lexiconsize += 1\n",
    "   # read in the vocabulary file\n",
    "    with open(\"vocab2.txt\", \"r\") as f:\n",
    "        data = f.readlines()\n",
    "   # Process the file a line at a time.\n",
    "    for line in data:\n",
    "        # The count is the first 3 characters\n",
    "        count = int(line[0:4])\n",
    "        # The word is the rest of the string\n",
    "        token = line[5:-1]\n",
    "       # Create a feature if itâ€™s appeared at least twice\n",
    "        if count > 1: \n",
    "            vocab_dict[token] = lexiconsize\n",
    "            lexiconsize += 1\n",
    "    # squirrel away the total size for later reference\n",
    "    vocab_dict[\"@size\"] = lexiconsize\n",
    "    return(vocab_dict)\n",
    "  \n",
    "vocab_dict = readvocab()\n",
    "\n",
    "# Turn string str into a vector.\n",
    "def tokenize(string, vocab_dict):\n",
    "  # initially the vector is all zeros\n",
    "  vec = [0 for i in range(vocab_dict[\"@size\"])]\n",
    "  # for each word\n",
    "  for t in string.split(\" \"):\n",
    "   # if the word has a feature, add one to the corresponding feature\n",
    "    if t in vocab_dict: vec[vocab_dict[t]] += 1\n",
    "   # otherwise, count it as an unk\n",
    "    else: vec[vocab_dict[\"@unk\"]] += 1\n",
    "  return(vec)\n",
    "\n",
    "# read in labeled examples and turn the strings into vectors\n",
    "def getdat(vocab_dict):\n",
    "    with open(\"cb.txt\", \"r\") as f:\n",
    "        data = f.readlines()\n",
    "    dat = []\n",
    "    labs = []\n",
    "    for line in data:\n",
    "        labs = labs + [int(line[0])]\n",
    "        dat = dat + [tokenize(line[2:], vocab_dict)]\n",
    "    return(dat, labs)\n",
    "\n",
    "(dat, labs) = getdat(vocab_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "53UHLTOiSdXl"
   },
   "source": [
    "We define two additional helper functions to make our recommendations:\n",
    "\n",
    "- `playgame` makes `rounds` recommendations using `chooser` and is given a `score`.  For `alpha` days the selections are random.\n",
    "\n",
    "- `argmax` returns the index from `indices` associated wiht the item in the `vals` list with the highest value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H2s0aTNrZGL0"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "def playgame(chooser, rounds, alpha):\n",
    "  curitem = 0\n",
    "  score = 0\n",
    "  trainset = []\n",
    "  trainlabs = []\n",
    "  b = 5\n",
    "  clf = MultinomialNB()\n",
    "\n",
    "  while curitem < rounds:\n",
    "    chosenitem = chooser(curitem, b, trainset, trainlabs, alpha, clf)\n",
    "    score = score + labs[chosenitem]\n",
    "    trainset = trainset + [dat[chosenitem]]\n",
    "    trainlabs = trainlabs + [labs[chosenitem]]\n",
    "    curitem += b\n",
    "  return(score)\n",
    "\n",
    "def argmax(indices, vals):\n",
    " best = max(vals)\n",
    " for i in range(len(indices)):\n",
    "   if vals[i] == best: \n",
    "     return(indices[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CrscWdZXWNzd"
   },
   "source": [
    "This function is our choosing function, `probachooser` and chooses between `b` options. `currentitem` is the initial item to consider.  `trainset` represents the results of previous selections. \n",
    "\n",
    "\n",
    "If we have not yet made `alpha` selections, the selection is random. \n",
    "\n",
    "If we have made `alpha` selections in the past, we fit our `clf` Naive Bayes model using the traing data of academic papers by title, `trainset`, and training labels for if the academic papers were interesting, `trainlabs`.  After we fit our `clf` model, we use it to select the item most likely to be labeled as interesting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dlCqy7KhZGP9"
   },
   "outputs": [],
   "source": [
    "def probachooser(curitem, b, trainset, trainlabs, alpha, clf):\n",
    "  if len(trainset) == alpha:\n",
    "    clf = clf.fit(trainset, trainlabs)\n",
    "#comment?\n",
    "  if len(trainset) < alpha:\n",
    "    chosenitem = random.randint(curitem,curitem+b-1)\n",
    "  else:\n",
    "    yhat = clf.predict_proba(dat[curitem:(curitem+b)])\n",
    "    chosenitem = argmax(range(curitem,curitem+b), [p for (c,p) in yhat])\n",
    "  return(chosenitem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dt6mp5vYdS6I"
   },
   "source": [
    "We will see how the number of rounds with random choices, `alpha` affects the final score.  We will run our `playgame` function with `alpha` values ranging from 10 to 200.  We will plot our scores below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "executionInfo": {
     "elapsed": 10934,
     "status": "ok",
     "timestamp": 1593607861232,
     "user": {
      "displayName": "Michael Littman",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh1OI1zp1NevMSZ87UN47k8bTZ--Ijr4v3ORV33Vg=s64",
      "userId": "06751891446459829367"
     },
     "user_tz": 240
    },
    "id": "kjaJpElLZGSz",
    "outputId": "bc2c5ce0-d380-439c-af01-89172a35fd97"
   },
   "outputs": [],
   "source": [
    "alphas = range(10,200,5)\n",
    "ress = []\n",
    "for alpha in alphas:\n",
    "  res = playgame(probachooser, 1000, alpha)\n",
    "  ress += [res]\n",
    "  \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.scatter(alphas, ress)\n",
    "plt.plot(alphas, ress)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zk6cY0iQdkqS"
   },
   "source": [
    "We rewrite our `probachooser` function so that `alpha` is now used as a smoothing parameter for our Naive Bayes model, ranging between 0 and 1.  We train our Naive Bayes classifier on every data element using our smoothing parameter in our Naive Bayes model, `alpha`. The `chosenitem` returned is just the one our classifier thinks is most likely to be interesting.\n",
    "\n",
    "We also will rewrite our `playgame` function to accomodate the changes we made in `probachooser`.\n",
    "\n",
    "Given these changes, we will plot how our score changes as we vary `alpha` from 0.00005 to 1. Note: Can take 15-20 minutes to run!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tFWoqjQcZGV0"
   },
   "outputs": [],
   "source": [
    "def probachooser(curitem, b, trainset, trainlabs, alpha):\n",
    "  if len(trainset) == 0:\n",
    "    chosenitem = random.randint(curitem,curitem+b-1)\n",
    "  else:\n",
    "    clf = MultinomialNB(alpha=alpha)\n",
    "    clf = clf.fit(trainset, trainlabs)\t\n",
    "    yhat = clf.predict_proba(dat[curitem:(curitem+b)])\n",
    "    chosenitem = argmax(range(curitem,curitem+b), [p for (c,p) in yhat])\n",
    "  return(chosenitem)\n",
    "\n",
    "def playgame(chooser, rounds, alpha):\n",
    "  curitem = 0\n",
    "  score = 0\n",
    "  trainset = []\n",
    "  trainlabs = []\n",
    "  b = 5\n",
    "\n",
    "  while curitem < rounds:\n",
    "    chosenitem = chooser(curitem, b, trainset, trainlabs, alpha)\n",
    "    score = score + labs[chosenitem]\n",
    "    trainset = trainset + [dat[chosenitem]]\n",
    "    trainlabs = trainlabs + [labs[chosenitem]]\n",
    "    curitem += b\n",
    "  return(score)\n",
    "\n",
    "rep = 10\n",
    "alphas = [0.00005, 0.0001, 0.0002, 0.0005, 0.001, 0.002, 0.005, 0.01, 0.02, 0.05, 0.1, 0.2, 0.5, 1.0]\n",
    "ress = []\n",
    "mins = []\n",
    "maxs = []\n",
    "for alpha in alphas:\n",
    "  print(\"Processing \" + alpha)\n",
    "  total = 0\n",
    "  res = []\n",
    "  for i in range(rep):\n",
    "    res += [playgame(probachooser, 1000, alpha)]\n",
    "  ress += [sum(res)/rep]\n",
    "  mins += [min(res)]\n",
    "  maxs += [max(res)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bXWNnxLbm10s"
   },
   "source": [
    "We plot the results of varying our smoothing parameter `alpha` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "executionInfo": {
     "elapsed": 1282151,
     "status": "ok",
     "timestamp": 1593609219453,
     "user": {
      "displayName": "Michael Littman",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh1OI1zp1NevMSZ87UN47k8bTZ--Ijr4v3ORV33Vg=s64",
      "userId": "06751891446459829367"
     },
     "user_tz": 240
    },
    "id": "TB0MKJK6ZGZO",
    "outputId": "93b7c678-9136-4dc0-bf34-0c187c3a46b2"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.scatter(alphas, ress)\n",
    "plt.plot(alphas, ress)\n",
    "# plt.fill_between(alphas, mins, maxs, alpha=0.6)   \n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "L12.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
