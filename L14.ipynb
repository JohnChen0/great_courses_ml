{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8WYu7l8dRtQ-"
   },
   "source": [
    "This notebook uses a deep learning image classification model, VGG16, to find similar images among a set of photos of household tools, cheese graters and foot files.\n",
    "\n",
    "Below we import the libraries we'll be using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4755,
     "status": "ok",
     "timestamp": 1590502365694,
     "user": {
      "displayName": "JZ Forde",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiwSogjDlg0BxeyyCUFJ3t-a6Mt210NSXjrBuLMYw=s64",
      "userId": "11263008094098858557"
     },
     "user_tz": 240
    },
    "id": "z2DFWnWNyqze",
    "outputId": "54b1a909-b532-49b2-9fab-901c94d64b17"
   },
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Dense,Flatten\n",
    "from keras.applications import vgg16\n",
    "from keras import backend as K\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DNlMft7quA70"
   },
   "source": [
    "We'll download the model VGG16 that has already been trained on the dataset ImageNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 37687,
     "status": "ok",
     "timestamp": 1590087039553,
     "user": {
      "displayName": "Michael Littman",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh1OI1zp1NevMSZ87UN47k8bTZ--Ijr4v3ORV33Vg=s64",
      "userId": "06751891446459829367"
     },
     "user_tz": 240
    },
    "id": "R6e0fdZ2yYBL",
    "outputId": "c70f2012-0150-4273-af5b-00b0cfcb6107"
   },
   "outputs": [],
   "source": [
    "model = vgg16.VGG16(weights='imagenet', include_top=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "y5i4lT8tURcT"
   },
   "source": [
    "The VGG16 model has 16 trainable layers, plus a few non-trainable layers.\n",
    "Its input is a 224x224x3 tensor, and its output is a one-dimensional tensor of size 1000.\n",
    "Use `model.summary()` to see a summary of its layers.\n",
    "\n",
    "Next, we'll remove the last two layers of the model,\n",
    "corresponding to the layers that use recognized features to generate classification.\n",
    "After the removal, the output is a one-dimensional tensor of size 4096."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P-5cbD72zKnw"
   },
   "outputs": [],
   "source": [
    "model2 = Model(model.input, model.layers[-2].output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "B0wL-7mKuLue"
   },
   "source": [
    "We'll extract addtional images below.\n",
    "The following code block extracts 20 image files to the current directory.\n",
    "\n",
    "* cg01.jpeg through cg10.jpeg contain images of cheese graters.\n",
    "* ff01.jpeg through ff10.jpeg contain images of foot files.\n",
    "\n",
    "These images have size 1080x720, but we will downsize them to 224x224 below\n",
    "to be compatible with the VGG16 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 697
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 14378,
     "status": "ok",
     "timestamp": 1590089841356,
     "user": {
      "displayName": "Michael Littman",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh1OI1zp1NevMSZ87UN47k8bTZ--Ijr4v3ORV33Vg=s64",
      "userId": "06751891446459829367"
     },
     "user_tz": 240
    },
    "id": "8XdXSJxHvOBP",
    "outputId": "61b4aa8f-8fc9-49ef-e60b-2247e0de40d0"
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "with zipfile.ZipFile('imgs/tools.zip') as f:\n",
    "    f.extractall()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GIvRfL5QaTrv"
   },
   "source": [
    "We will then load and process the images so that we can input them into our model and use the predictions from the model to find pairs of images that are most similar to each other according to our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 27932,
     "status": "ok",
     "timestamp": 1590089905945,
     "user": {
      "displayName": "Michael Littman",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh1OI1zp1NevMSZ87UN47k8bTZ--Ijr4v3ORV33Vg=s64",
      "userId": "06751891446459829367"
     },
     "user_tz": 240
    },
    "id": "bQ1GesmY9mDf",
    "outputId": "a81149f7-625e-495f-c47a-46bf09407fa6"
   },
   "outputs": [],
   "source": [
    "# get images\n",
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "\n",
    "# dat will contain 20 one-dimensional numpy.ndarray, each of size 4096,\n",
    "# corresponding to the 4096 features output by the truncated VGG16 model.\n",
    "dat = []\n",
    "# imgs will contain 20 PIL.Image.Image objects.\n",
    "imgs = []\n",
    "imgflist = [\"cg01\", \"cg02\", \"cg03\", \"cg04\", \"cg05\", \"cg06\", \"cg07\", \"cg08\", \"cg09\", \"cg10\",\n",
    "           \"ff01\", \"ff02\", \"ff03\", \"ff04\", \"ff05\", \"ff06\", \"ff07\", \"ff08\", \"ff09\", \"ff10\"]\n",
    "for imgf in imgflist:\n",
    "    img = image.load_img(imgf+\".jpeg\", target_size=(224,224))\n",
    "    imgs.append(img)\n",
    "    img_arr = np.expand_dims(image.img_to_array(img), axis=0)\n",
    "    x = preprocess_input(img_arr)\n",
    "    preds = model2.predict(x)\n",
    "    dat.append(preds[0])\n",
    "for i in range(len(dat)):\n",
    "  i1 = dat[i]\n",
    "  bestmatch, bestsim = -1, 0\n",
    "  for j in range(len(dat)):\n",
    "    i2 = dat[j]\n",
    "    sim = i1 @ i2\n",
    "    if sim > bestsim and i != j: bestmatch, bestsim = j, sim\n",
    "    print(j, sim)\n",
    "  print(i, bestmatch, bestsim)\n",
    "  display(imgs[i], imgs[bestmatch])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5L3U7eNlbA6J"
   },
   "source": [
    "Below are the number of images we have downloaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 405,
     "status": "ok",
     "timestamp": 1590089961100,
     "user": {
      "displayName": "Michael Littman",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh1OI1zp1NevMSZ87UN47k8bTZ--Ijr4v3ORV33Vg=s64",
      "userId": "06751891446459829367"
     },
     "user_tz": 240
    },
    "id": "GZ7WC0je9mJN",
    "outputId": "fcca29e6-d2a5-4b08-e8a1-a52c7b52f209"
   },
   "outputs": [],
   "source": [
    "len(dat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Pj_J81yTbiwc"
   },
   "source": [
    "We will also do this by dividing the data into training and testing data, and for each image in the training set, find the image in the test set that is most similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 8607,
     "status": "ok",
     "timestamp": 1590090050385,
     "user": {
      "displayName": "Michael Littman",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh1OI1zp1NevMSZ87UN47k8bTZ--Ijr4v3ORV33Vg=s64",
      "userId": "06751891446459829367"
     },
     "user_tz": 240
    },
    "id": "GCBVnUlO3p8X",
    "outputId": "7403f45e-f0fb-4197-e346-f96c9aacbede"
   },
   "outputs": [],
   "source": [
    "train = [i for i in range(0,5)]+[i for i in range(10,15)]\n",
    "test = [i for i in range(5,10)]+[i for i in range(15,20)]\n",
    "\n",
    "for i in test:\n",
    "  bestj = -1\n",
    "  bestdist = 0\n",
    "  for j in train:\n",
    "    if i != j: \n",
    "#      dist = sum((dat[i] - dat[j])**2)\n",
    "      dist = (dat[i] @ dat[j])**2 / ((dat[i] @ dat[i])*(dat[j] @ dat[j])) \n",
    "      if dist > bestdist: bestj, bestdist = j, dist\n",
    "  print(f\"test image {i} best matches train image {bestj}\")\n",
    "  display(imgs[i])\n",
    "  display(imgs[bestj])\n",
    "  print(\" \")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "L14.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
